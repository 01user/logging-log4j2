<?xml version="1.0"?>
<!--
    Licensed to the Apache Software Foundation (ASF) under one or more
    contributor license agreements.  See the NOTICE file distributed with
    this work for additional information regarding copyright ownership.
    The ASF licenses this file to You under the Apache License, Version 2.0
    (the "License"); you may not use this file except in compliance with
    the License.  You may obtain a copy of the License at

         http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
-->

<document xmlns="http://maven.apache.org/XDOC/2.0"
          xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
          xsi:schemaLocation="http://maven.apache.org/XDOC/2.0 http://maven.apache.org/xsd/xdoc-2.0.xsd">
  <properties>
    <title>Performance</title>
    <author email="rpopma@apache.org">Remko Popma</author>
    <author email="rgoers@apache.org">Ralph Goers</author>
  </properties>

  <body>
    <section name="Performance">
      <!--
      <p>One of the often-cited arguments against logging is its
        computational cost. This is a legitimate concern as even moderately
        sized applications can generate thousands of log requests. Much
        effort was spent measuring and tweaking logging performance. Log4j
        claims to be fast and flexible: speed first, flexibility second.
      </p>
      -->
      <p>Apart from functional requirements, an important reason for selecting a logging library is often how well it
        fulfills non-functional requirements like reliability and performance.</p>
      <p>On this page we will compare the performance of a number of logging frameworks
        (java.util.logging "JUL", Logback, Log4j 1.2 and Log4j 2.6),
        and document some performance trade-offs for Log4j 2 functionality.
      </p>
      <h2>Benchmarks</h2>
      <p>Performance can mean different things to different people. Common terms in this context are
        throughput and latency: <em>Throughput</em> is a measure of capacity and can be expressed in a single number:
        how many messages can be logged in a certain period of time.
        <em>Response time latency</em> is how long it takes to log a message.
        This cannot be expressed in a single number because each measurement has its own
        response time and we are often most interested in the outliers: how many there were and how large they were.</p>
      <p>We will measure and compare three performance indicators: </p>
      <ul>
        <li><b>Peak throughput</b> is the maximum throughput measured over a short period of time.
          This number is useful for systems that need to log bursts of messages followed by periods of
          relative quiet. Many systems that react to external events behave like this,
          but for systems that need to log a lot at a constant high rate (for example, batch jobs)
          this is less likely to be a useful measure of performance.</li>
        <li><b>Maximum sustained throughput</b> is the throughput averaged over a long time.
          This is a useful measure of the "upper limit" of the capacity of the logging library.
          It is not recommended that reactive applications actually log at this rate since under this load
          they will likely experience jitter and large response time spikes.</li>
        <li><p>
          <b>Response time</b> is the total amount of time it takes to log a message and is the sum of the
          service time and wait time. The service time is the time it takes to do the work to log the message.
          As the workload increases, the service time often varies little:
          to do X amount of work it always takes X amount of time.
          The wait time is how long the request had to wait in a queue before being serviced.
          <em>As the workload increases, wait time often grows to many times the service time.</em>
          </p>
          <p>
            What is often measured and reported as <em>latency</em> is actually <em>service time</em>,
            which unfortunately hides the wait time.
            The below graph shows response time and service time of the same system under a load of 100,000 messages
            per second. Out of 25 million measurements, only ~50 are more than 200 microseconds, less than 0.001%.
            In a service time-only graph this would hardly be visible.
            However, the wait time adds up and the response time graph shows that in reality many more events are impacted
            by these delays.
          </p>
          <p><img src="images/ResponseTimeVsServiceTimeAsyncLoggers.png" /></p>
          <p>
            To learn more, watch Gil Tene's
            <a href="http://www.infoq.com/presentations/latency-response-time">How NOT to measure latency</a>
            (and prepare to be shocked).
          </p>
          </li>
      </ul>
      <h2>Logging Library Performance Comparison</h2>

      <h3>Asynchronous Logging - Peak Throughput Comparison</h3>
      <p>Asynchronous Logging is useful to deal with bursts of events. How this works is that
        a minimum amount of work is done by the application thread to capture all required information in a log event,
        and this log event is then put on a queue for later processing by a background thread.
        As long as the queue is sized large enough, the application threads should be able to spend
        very little time on the logging call and return to the business logic very quickly.
      </p>
      <p>It turns out that the choice of queue is extremely important for peak throughput.
        Log4j 2's Async Loggers use a
        <a href="http://lmax-exchange.github.com/disruptor/">lock-free data structure</a>, whereas
        Logback, Log4j 1.2 and Log4j 2's Asynchronous Appenders use an ArrayBlockingQueue.
        With a blocking queue, multi-threaded applications often experience lock contention when trying to
        enqueue the log event.
      </p>
      <p>The graph below illustrates the difference in peak throughput.
        For details, see the <a href="manual/async.html">Async
        Loggers</a> manual page.</p>
      <p><img src="images/async-throughput-comparison.png" alt="Peak throughput comparison" />
      </p>
      <p>The results above were obtained with log4j-2.0-beta5, disruptor-3.0.0.beta3,
        log4j-1.2.17 and logback-1.0.10.
      </p>

      <h3>Synchronous File Logging - Sustained Throughput Comparison</h3>
      <p>The graph below compares Log4j 2.6 in garbage-free mode to Log4j 2.6 "classic" mode (which allocates
        temporary objects for every logging call), Log4j 1.2.17, Logback 1.1.7 and
        Java util logging (JUL) on Oracle Java 1.8.0_45. The Log4j 2.x results use the RandomAccessFile appender.
        Log4j 1.2.17, Logback and JUL use their respective File appenders. ImmediateFlush was set to <tt>false</tt> for all
        loggers that support this. The JUL results are for the <tt>XMLFormatter</tt> (which in our measurements was
        about twice as fast as the <tt>SimpleFormatter</tt>).</p>
      <p>Our test results suggest that the throughput of the other logging frameworks we tested
        will rapidly decline in multi-threaded applications.</p>
      <p><img src="images/SyncThroughputLoggerComparisonLinux.png" /></p>
      <p>The synchronous logging throughput results above are obtained with the
        <a href="http://openjdk.java.net/projects/code-tools/jmh/">JMH</a> Java benchmark harness.
        See the <tt>org.apache.logging.log4j.perf.jmh.FileAppenderBenchmark</tt> source code in the log4j-perf module.</p>

      <h3>Synchronous File Logging - Response Time Comparison</h3>

      <h3>Synchronous Socket Sustained Throughput Comparison</h3>
      <h3>Synchronous Syslog Sustained Throughput Comparison</h3>

      <h2>Trade-offs</h2>

      <!--
      <p>The user should be aware of the following performance issues.</p>
          <h3>Logging performance when logging is turned off.</h3>
          <p>When logging is turned off entirely or just for a set of Levels, the cost of a log request consists of
            two method invocations plus an integer comparison. On a 2.53 GHz Intel Core 2 Duo MacBook Pro
            calling isDebugEnabled 10 million times produces an average result in nanoseconds of:</p>
            <pre>
            Log4j: 4
            Logback: 5
            Log4j 2: 3
            </pre>
          <p>
            The numbers above will vary slightly from run to run so the only conclusion that should be
            drawn is that all 3 frameworks perform similarly on this task.
          </p>
          <p>However, The method invocation involves the "hidden" cost of parameter construction.
          </p>
          <p>For example,
          </p>
            <pre>
              logger.debug("Entry number: " + i + " is " + String.valueOf(entry[i]));
            </pre>
          <p>
            incurs the cost of constructing the message parameter, i.e. converting both integer
            <code>i</code> and <code>entry[i]</code> to a String, and concatenating intermediate strings,
            regardless of whether the message will be logged or not.

            This cost of parameter construction can be quite high and it
            depends on the size of the parameters involved.

            A comparison run on the same hardware as above yields:
          </p>
            <pre>
            Log4j: 188
            Logback: 183
            Log4j 2: 188
            </pre>
          <p>
            Again, no conclusion should be drawn regarding relative differences between the frameworks on
            this task, but it should be obvious that it is considerably more expensive than simply testing
            the level.
          </p>
          <p>
            The best approach to avoid the cost of parameter construction is to use Log4j 2's formatting
            capabilities. For example, instead of the above write:
          </p>
            <pre>
            logger.debug("Entry number: {} is {}", i, entry[i]);
            </pre>
          <p>
            Using this approach, a comparison run again on the same hardware produces:
          </p>
            <pre>
            Log4j: Not supported
            Logback: 9
            Log4j 2: 4
            </pre>
          <p>
            These results show that the difference in performance between the call to isDebugEnabled and
            logger.debug is barely discernible.
          </p>
          <p>In some circumstances one of the parameters to logger.debug will be a costly method call that
            should be avoided if debugging is disabled. In those cases write:
          </p>
            <pre>
            if(logger.isDebugEnabled() {
                logger.debug("Entry number: " + i + " is " + entry[i].toString());
            }
            </pre>
          <p>This will not incur the cost of whatever the toString() method needs to do if debugging is disabled.
            On the other hand, if the logger is enabled for the debug level, it will incur twice the cost of
            evaluating whether the logger is enabled or not: once
            in <code>isDebugEnabled</code> and once in <code>debug</code>. This is an insignificant
            overhead because evaluating a logger takes about 1% of the time it takes to actually log.
          </p>
          <p>Certain users resort to pre-processing or compile-time
            techniques to compile out all log statements. This leads to perfect
            performance efficiency with respect to logging. However, since the
            resulting application binary does not contain any log statements,
            logging cannot be turned on for that binary. This seems to be
            a disproportionate price to pay in exchange for a small performance
            gain.
          </p>
          <h3>The performance of deciding whether to log or not to log when logging is turned on.</h3>
          <p>
            Unlike Log4j, Log4j 2 Loggers don't "walk a hierarchy". Loggers point directly to the
            Logger configuration that best matches the Logger's name. This incurs extra overhead when the Logger
            is first created but reduces the overhead every time the Logger is used.
          </p>
          <h3>Actually outputting log messages</h3>
          <p>This is the cost of formatting the log output and sending it to its target destination. Here again,
            a serious effort was made to make layouts (formatters) perform as quickly as possible. The same
            is true for appenders. One of the fundamental tenets of Log4j 2 is to use immutable objects whenever
            possible and to lock at the lowest granularity possible. However, the cost of actually formatting and
            delivering log events will never be insignificant. For example, the results of writing to a simple log
            file using the same format using Log4j, Logback and Log4j 2 are:
          </p>
          <pre>
              Log4j: 1651
              Logback: 1419
              Log4j 2.0: 1542
          </pre>
          <p>
            As with many of the other results on this page the differences between the frameworks above should be
            considered insignificant. The values will change somewhat on each execution and changing the order the
            frameworks are tested or adding calls to System.gc() between the tests can cause a variation in the
            reported times. However, these results show that actually writing out the events can be at least 1000
            times more expensive than when they are disabled, so it is always recommended to take advantage of
            Log4j 2's fine-grained filtering capabilities.
          </p>
        -->
          <h3>Advanced Filtering</h3>
          <p>
            Both Logback and Log4j 2 support advanced filtering. Logback calls them TurboFilters while
            Log4j 2 has a single Filter object. Advanced filtering provides the capability to filter
            LogEvents using more than just the Level before the events are passed to Appenders.
            However, this flexibility does come with some cost. Since multi-threading can also have an impact on the
            performance of advanced filtering, the table below shows the difference in performance in two different
            sets of context-wide filters running on the same hardware as the previous tests using
            various numbers of threads.
          </p>
          <table>
            <tr>
              <th>Test</th>
              <th>1 thread</th>
              <th>2 threads</th>
              <th>5 threads</th>
              <th>10 threads</th>
              <th>20 threads</th>
              <th>50 threads</th>
            </tr>
            <tr>
              <td>Logback MDCFilter</td>
              <td>37</td>
              <td>50</td>
              <td>145</td>
              <td>316</td>
              <td>606</td>
              <td>1670</td>
            </tr>
            <tr>
              <td>Log4j 2 ThreadContextMapFilter</td>
              <td>30</td>
              <td>35</td>
              <td>85</td>
              <td>165</td>
              <td>341</td>
              <td>864</td>
            </tr>
            <tr>
              <td>Logback MarkerFilter</td>
              <td>17</td>
              <td>24</td>
              <td>59</td>
              <td>115</td>
              <td>234</td>
              <td>547</td>
            </tr>
             <tr>
              <td>Log4j 2 MarkerFilter</td>
              <td>4</td>
              <td>5</td>
              <td>7</td>
              <td>20</td>
              <td>35</td>
              <td>92</td>
            </tr>
          </table>

          <h3>Client vs Server</h3>
          <p>
            Java supports a "client" and a "server" mode of operation. By default, applications that are
            run in Java 5 on machines with 2 CPUs or more and 2GB of memory or more on operating systems
            other than Windows or are run in a 64-bit JVM are automatically configured to run in "server" mode.
            Testing has shown that Log4j 2 benefits greatly from running in server mode and user's are
            strongly encouraged to configure their applications to run in server mode when using Log4j 2.
          </p>


      <p>
        The performance results above were all derived from running the DebugDisabledPerformanceComparison,
        FilterPerformanceComparison, and PerformanceComparison junit tests which can be found in the
        Log4j 2 unit test source directory.
      </p>
    </section>
  </body>
</document>
